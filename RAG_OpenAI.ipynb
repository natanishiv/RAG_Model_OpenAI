{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfRUc5QZEr7K"
   },
   "source": [
    "## Creating **Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_0ibqwpLBlo"
   },
   "outputs": [],
   "source": [
    "pip install OpenAI==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XixNWJ3NMEf0"
   },
   "outputs": [],
   "source": [
    "pip install pinecone-client==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uKWYY1hkEqdk"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client=OpenAI(\n",
    "    api_key=\"\"                      #Enter your api key here\n",
    ")\n",
    "MODEL = \"text-embedding-3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zU4qCoo-EnFQ"
   },
   "outputs": [],
   "source": [
    "# ASsuming dataset is in form of question answer pairs\n",
    "dataset=[\n",
    "    (\"Where is company headquartered ? \", \"India\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVsBq7XyGtXm"
   },
   "outputs": [],
   "source": [
    "embeddings={}\n",
    "for question,answer in dataset:\n",
    "\n",
    "  res=client.embeddings.create(\n",
    "      input=[question],\n",
    "      model=MODEL\n",
    "  )\n",
    "\n",
    "  question_embedding = res.data[0].embedding\n",
    "  embeddings[question] = {question_embedding}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWn4xPaeKkSa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEXxNNUQKwAR"
   },
   "source": [
    "## **Intializing Pinecone Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "a9pRIDHEN_-c"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWdVQI8CKvDv"
   },
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=\"\")\n",
    "\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"ap-south-1\")\n",
    "\n",
    "index_name = 'semantic-search-openai'\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[qa_pairs[0]]),\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqQZgCeoYRT6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWBEjLc6YRQU"
   },
   "source": [
    "## **Upsert data to pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMFCBaRuYWqX"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set batch size for processing\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize count for creating unique IDs\n",
    "count = 0\n",
    "\n",
    "# Iterate through the dataset in batches\n",
    "for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "    # Set end position of batch\n",
    "    i_end = min(i + batch_size, len(dataset))\n",
    "\n",
    "    # Get batch of question-answer pairs and their IDs\n",
    "    batch_dataset = dataset[i: i_end]\n",
    "    ids_batch = [str(n) for n in range(count, count + len(batch_dataset))]\n",
    "\n",
    "    # Extract embeddings and metadata for the batch\n",
    "    batch_embeddings = [embeddings[qa_pair[0]] for qa_pair in batch_dataset]\n",
    "    batch_metadata = [{'question': qa_pair[0], 'answer': qa_pair[1]} for qa_pair in batch_dataset]\n",
    "\n",
    "    # Upsert batch to Pinecone index\n",
    "    index.upsert(vectors=list(zip(ids_batch, batch_embeddings, batch_metadata)))\n",
    "\n",
    "    # Update count for creating unique IDs\n",
    "    count += len(batch_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dRWO2NPYYR6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWooPpLoaYpx"
   },
   "source": [
    "## **Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WA_U4DLnaaf5"
   },
   "outputs": [],
   "source": [
    "query = \"WHat does Yardstick AI do and where it is headquartered\"\n",
    "\n",
    "xq = client.embeddings.create(input=query, model=MODEL).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXhhH_1Vafg2"
   },
   "outputs": [],
   "source": [
    "res = index.query([xq], top_k=2, include_metadata=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzgjmL6SaiBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej88Q3HVcy_J"
   },
   "source": [
    "## **Generating Respnse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jiiz3VGxc2Ji"
   },
   "outputs": [],
   "source": [
    "# Stack the top 2 matching question-answer pairs together\n",
    "stacked_pairs = \"\\n\\n\".join([f\"Question: {result.metadata['question']}\\nAnswer: {result.metadata['answer']}\" for result in res.results])\n",
    "\n",
    "# Initialize conversation context with the query and stacked pairs\n",
    "\n",
    "\n",
    "conversation_context = f\"User: Hey GPT, this is my query: {query}\\nAI: Based on the following question-answer pairs:\\n{stacked_pairs}\\nUser: give me a short response to this query.\\n\"\n",
    "\n",
    "\n",
    "# Call the OpenAI API to generate a response\n",
    "response = client.Completion.create(\n",
    "    model=MODEL,\n",
    "    prompt=conversation_context,\n",
    "    temperature=0.7,\n",
    "    max_tokens=70\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(\"AI Response:\", response.choices[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMz0CW-_g8Jd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
